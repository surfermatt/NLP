from flask import Flask, request, jsonify
from sentence_transformers import SentenceTransformer, util
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch
import numpy as np
import pandas as pd

import os
import requests
import zipfile

def download_and_unpack_model():
    model_url = "https://vh297-fm.sweb.ru/files/surfermatt_ru/ML/projectNLP/models/rut5_model"
    model_dir = "models/rut5_model"

    if not os.path.exists(model_dir):
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
        r = requests.get(model_url)
        with open("rut5_model.zip", "wb") as f:
            f.write(r.content)
        with zipfile.ZipFile("rut5_model.zip", 'r') as zip_ref:
            zip_ref.extractall("models")
        os.remove("rut5_model.zip")
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–∞.")

download_and_unpack_model()

app = Flask(__name__)

download_and_unpack_model()
tokenizer = T5Tokenizer.from_pretrained("models/rut5_model")
generator = T5ForConditionalGeneration.from_pretrained("models/rut5_model")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
retriever = SentenceTransformer('models/sentence_model')
#generator = T5ForConditionalGeneration.from_pretrained('models/rut5_model')
#tokenizer = T5Tokenizer.from_pretrained('models/rut5_model')

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
corpus = pd.read_csv('models/journal_texts.csv')['description'].tolist()
corpus_embeddings = np.load('models/journal_embeddings.npy')
corpus_embeddings = torch.tensor(corpus_embeddings)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    query = data.get('description', '')

    if not query:
        return jsonify({'error': 'No description provided'}), 400

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    query_embedding = retriever.encode(query, convert_to_tensor=True)
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=1)[0]
    best_match_idx = hits[0]['corpus_id']
    best_match_text = corpus[best_match_idx]

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    input_text = f'–Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—å: {best_match_text} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:'
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    output_ids = generator.generate(input_ids, max_length=100)
    recommendation = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    return jsonify({
        'matched_issue': best_match_text,
        'recommendation': recommendation
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
